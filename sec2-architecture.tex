\section{The Architecture of the Science Platform}\label{architecture}

This section presents the overall architecture of the LSST Science Platform,
including its major components and interfaces and design principles.

\subsection{Design Overview}\label{design-overview}

The LSP is a multi-tier architecture with distinct but inter-related
user-facing "Aspects" over an underlying layer of services that in turn
rests on computational infrastructure.

\subsubsection{Functional Architecture}\label{functional-architecture}

The Portal Aspect, JupyterLab Aspect, and Web APIs Aspects provide three
distinct user interfaces.  Internally, the LSP interfaces with the Data
Backbone via the Data Butler client interface and direct "native" Data Backbone
interfaces to provide access to the Science Data Archive and (for some
instances) other data products.  The LSP also has its own data storage for
Portal configurations, user notebooks, and other user files and databases.  It
interfaces with compute provisioning services, authentication and authorization
services, and resource management, including a proposal management system to
handle requests for resources.  Networking connects the LSP with internal
systems and the external world and provides a measure of cybersecurity.

The Portal Aspect is implemented using the language-agnostic Web APIs to
retrieve data.  The JupyterLab Aspect can also use the Web APIs if the user
desires, either through a VO client or directly as a Web service.

\subsubsection{Deployment Architecture}\label{deployment-architecture}

The LSP is designed to be deployable in multiple locations within the LSST DMS.
The production instances are the US Data Access Center, the Chilean Data Access
Center, the Science Validation instance in the NCSA Enclave, and the
Commissioning Cluster instance in the Base Enclave.  Additional instances are
used for integration testing in the Integration environment and for science
pipelines developer usage in the Developer environment.  LSP developers will
of course instantiate components of the LSP during their own development.

The LSP is composed of multiple services that are containerized and deployed
using Kubernetes.  The underlying provisioning of compute resources is handled
by the Provisioning and Deployment Service component of the LSST DMS.  The
technology to be used for provisioning in this component is being evaluated and
may vary between instances.  vSphere, HTCondor, SLURM, and OpenStack are all
potential alternatives.  Provisioning is elastic: JupyterLab dynamically
instantiates notebooks on demand, and the Portal and Web API services can be
expanded or contracted as needed.  This elasticity and fault tolerance
considerations require that the services hold minimal non-persistent state so
that loss of a single service instance has correspondingly minimal impact on
users.

\subsection{Data Access and Storage}\label{data-access-and-storage}


\subsubsection{Databases}\label{databases}

\paragraph{Database content overview}\label{database-content-overview}

\begin{itemize}
\item Image and Visit metadata
\item Catalogs
\item Composite data
\item Observatory metadata (including EFD)
\item Reference catalogs
\end{itemize}

\paragraph{Database technologies}\label{database-technologies}

\begin{itemize}
\item Conventional
\item Qserv
\end{itemize}

\subsubsection{Images}\label{images}

\subsubsection{LSST-specific Objects}\label{lsst-specific-objects}

("ORM-ish" behavior, composite objects, etc.)

\subsubsection{Data Access Services}\label{data-access-services}

(This is a description of the functional architecture of the services; a detailed description of the APIs offered by the services is in section 5 below)

\subsubsection{User Catalog Data Support}\label{user-catalog-data-support}

("Level 3 catalogs")

\subsubsection{User Workspace Storage}\label{user-workspace-storage}

(Generic file-oriented storage, assumed to also support "Level 3 image data", e.g., custom coadds)

\subsubsection{Data Access Permissions and Quotas}\label{data-access-permissions-and-quotas}

\subsubsection{Support of Previous Releases}\label{support-of-previous-releases}

(This could go elsewhere in the outline hierarchy)

\subsection{Computing Resources}\label{computing-resources}

The LSP provides access to computing resources to enable users to analyze
LSST data products, alone and in combination with user-provided data.

\subsubsection{Basic user compute services}\label{basic-user-compute-services}

Users of the Portal Aspect share computing resources allocated to the Portal as
a whole.  The same strategy is used for the Web APIs.  Within those shared
resources, per-user accounting is performed for resources such as queries and
temporary disk space so that resource management policies may be implemented
if required.

Each user of the JupyterLab Aspect is assigned a virtual machine per notebook.
The virtual machine can be selected from a menu of preconfigured installations
of LSST and other software, or users can customize a virtual machine and save
it for future use.  The resources allocated to the virtual machine are
defined by policy.

\subsubsection{Large-scale batch and parallel computing}\label{large-scale-batch-and-parallel-computing}

Users of the LSP (any Aspect) may trigger large-scale, long-latency,
asynchronous data processing.  This processing is handled by submitting
jobs to a batch system based on HTCondor.  Mechanisms are provided in the
three Aspects to monitor the status of batch jobs and retrieve their results.

\subsection{Authentication and Authorization}\label{authentication-and-authorization}

Identity management and associated roles and authorizations are provided by a
unified LSST system as described in LSE-279.  All uses of all instances of the
LSP, including the Web APIs Aspect, are authenticated.

\subsection{Resource Management}\label{resource-management}

LSP users in the DACs will receive a certain quota of resources (compute,
storage, query, bandwidth, etc.).  Usage beyond that quota will be allocated by
a committee.  Outside the LSP, a proposal management system will track requests
and allocations that will then be implemented by per-user resource management
features within each LSP component.

\subsection{Cybersecurity Considerations}\label{cybersecurity-considerations}

The Data Access Center instances of the LSP are the only ones exposed directly
to the public Internet.  The Science Validation, Commissioning Cluster,
Integration, and Developer instances are only reachable from LSST-internal
networks, helping to reduce the attack surface.  The DAC instances are isolated
(on a different LAN) from the production Level 1 and Level 2 domains within the
NCSA enclave.  Although they may share underlying computing resources, such
resources are provisioned for either production or DAC but not both at the
same time.  The LSST data products within the Data Backbone will be read-only
to the DAC LSP instances.  The Data Backbone interfaces are all authenticated,
but they will be used for some types of user data, so the Backbone must be
hardened against attack (or accident) from the LSP.  Provisioning separate
virtual machines per user in the JupyterLab Aspect helps to prevent unwanted
sharing of information between users.  The Portal Aspect and Web APIs Aspect
need to be hardened in the same ways as any Web-enabled service, including
sanitizing user inputs, avoiding cross-site scripting, etc.

\subsection{Additional Support Services}\label{additional-support-services}

